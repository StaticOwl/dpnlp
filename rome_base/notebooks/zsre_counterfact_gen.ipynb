{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43efaac-7257-41ed-bdf6-f63725f0a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"The attention mask and the pad token id were not set.*\")\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a6ef8e-81c2-4e67-9588-e000aa7a8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d227a5-46f8-4f96-a680-b5502716e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"google/flan-t5-xl\"\n",
    "TOKEN=''\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n",
    "DATASET = \"community-datasets/qa_zre\"\n",
    "BASE_PATH = '/scratch/gilbreth/dparveez/'\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c511e7a-ea4f-4a12-80ec-59d2f210449f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b483a8764c4160a880310fbe05e35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=BASE_PATH, token=TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    device_map=DEVICE, \n",
    "    torch_dtype=torch.float16,\n",
    "    token=TOKEN,\n",
    "    cache_dir=BASE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "120e84f4-dfe8-453b-9bd8-edcf52bde017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dparveez/.local/lib/python3.11/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET, split=\"train\", cache_dir=BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41657c36-45ce-4cd8-8140-5f07d01d82f2",
   "metadata": {},
   "source": [
    "## SAMPLE GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "970fc69c-2805-4a29-b16d-7dba3ade7f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "BASE_PROMPT = f\"\"\"Given a question with subect represented by 'XXX', a subject, and an answer to the question (target),\n",
    "reword the question as a fact, such that the relation between the subject and target is preserved.\n",
    "Then, modify the fact so that the subject remains the same, but the target changes to something incorrect, such that the relation between the subject\n",
    "and the new target is also preserved.\n",
    "Also, state the new answer explicitly as the new target, and the old answer as the previous target.\n",
    "Consider the examples below for reference. Be creative when chosing new answers/targets, but also be grounded in reality.\n",
    "    \n",
    "E.g. 1: - \n",
    "Question: What is the publication year of XXX?\n",
    "Subject: Porca vacca\n",
    "Target: 1982\n",
    "Fact: The publication year of Porca vacca is 1982.\n",
    "Modified Fact: The publication year of Porca vacca is 1997.\n",
    "New Target: 1997\n",
    "\n",
    "E.g. 2: -\n",
    "Question: Which continent is XXX located in?\n",
    "Subject: India\n",
    "Target: Asia\n",
    "Fact: India is located in the continent of Asia.\n",
    "Modified Fact: India is located in the continent of Europe.\n",
    "New Target: Europe\n",
    "\n",
    "E.g. 3: -\n",
    "Question: What programming language was used to write XXX?\n",
    "Subject: Caveman2\n",
    "Target: Lisp\n",
    "Fact: Caveman2 was programmed in Lisp.\n",
    "Modified Fact: Caveman2 was programmed in C#.\n",
    "New Target: C#\n",
    "\n",
    "E.g. 4: -\n",
    "Question: Which was the record label for XXX?\n",
    "Subject: Change of the Century\n",
    "Target: Atlantic Records\n",
    "Fact: The record label for Change of the Century was Atlantic Records.\n",
    "Modified Fact: The record label for Change of the Century was A&E Records.\n",
    "New Target: A&E Records\n",
    "\n",
    "E.g. 5: -\n",
    "Question: Who was the lead actor in XXX?\n",
    "Subject: The Terminator\n",
    "Target: Arnold Schwarzenegger\n",
    "Fact: The lead in The Terminator was actor Arnold Schwarzenegger.\n",
    "Modified Fact: The lead in The Terminator was actor Brad Pitt.\n",
    "New Target: Brad Pitt\n",
    "\n",
    "E.g. 6: -\n",
    "Question: Which footballer is famously associated with XXX?\n",
    "Subject: Hand of God Goal\n",
    "Target: Diego Maradona\n",
    "Fact: The Hand of God Goal is associated with footballer Diego Maradona.\n",
    "Modified Fact: The Hand of God Goal is associated with footballer Lionel Messi.\n",
    "New Target: Lionel Messi\n",
    "\"\"\"\n",
    "\n",
    "ORIG_PROMPT_LEN = len(BASE_PROMPT)\n",
    "\n",
    "QBS = 'Question: ' # Question Block Start\n",
    "SBS = 'Subject: '\n",
    "TBS = 'Target: '\n",
    "FBS = 'Fact: '\n",
    "MFBS = 'Modified Fact: '\n",
    "NTBS = 'New Target: '\n",
    "\n",
    "GENERATED_BLOCK_STARTS = [QBS, SBS, TBS, FBS, MFBS, NTBS]\n",
    "\n",
    "PLAIN_ENGLISH_REGEX = r\"[a-zA-Z0-9\\s'\\\".?]*\"\n",
    "\n",
    "# Determine if row from dataset is suitable to try and turn into a counteract.\n",
    "def validate_row(row):\n",
    "    try:\n",
    "        if not('XXX' in row['question'] and '?' in row['question']):\n",
    "            return False\n",
    "        if len(row['answers']) != 1:\n",
    "            return False\n",
    "        if not(len(row['answers'][0]) > 0 and len(row['subject']) > 0):\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def build_incomplete_info_block(row):\n",
    "    return f\"\"\"\n",
    "Question: {row['question']}\n",
    "Subject: {row['subject']}\n",
    "Target: {row['answers'][0]}\n",
    "\"\"\" \n",
    "\n",
    "# Given an incomplete info block (with Question, Subject and Target), generate other information.\n",
    "def generate_complete_info(model, tokenizer, info):\n",
    "    new_prompt = BASE_PROMPT + info\n",
    "\n",
    "    inputs = tokenizer(new_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=620, do_sample=False, temperature=0.8)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Generated information (output from generate_info()) has BASE_PROMPT, and possibly other irrelevant generated information blocks.\n",
    "# Extract only the generated information block that exists right after the BASE_PROMPT.\n",
    "def extract_info_block(generated_info_str):\n",
    "    info = generated_info_str[ORIG_PROMPT_LEN: ]\n",
    "    \n",
    "    lines = info.splitlines()\n",
    "    \n",
    "    line_index = 0\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith(NTBS):\n",
    "            line_index = i\n",
    "            break\n",
    "    \n",
    "    new_info = \"\\n\".join(lines[:line_index+1])\n",
    "    return new_info.strip()\n",
    "\n",
    "# Given the relevant generated info block, validate its correctness.\n",
    "def validate_info_and_extract_dict(info_str, row):\n",
    "    info_dict = {}\n",
    "    question, subject, target = row['question'], row['subject'], row['answers'][0]\n",
    "    \n",
    "    subject = subject.strip()\n",
    "    target = target.strip()\n",
    "\n",
    "    lines = info_str.splitlines()\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if not line.startswith(GENERATED_BLOCK_STARTS[i]):\n",
    "            return False, info_dict\n",
    "        \n",
    "        if line.startswith(QBS) and not(question in line):\n",
    "            return False, info_dict\n",
    "        \n",
    "        if line.startswith(SBS) and not(subject in line):\n",
    "            return False, info_dict\n",
    "        \n",
    "        if line.startswith(TBS):\n",
    "            if not(target in line):\n",
    "                return False, info_dict\n",
    "            \n",
    "            t = line[len(TBS) - 1: ]\n",
    "            t = t.strip()\n",
    "            \n",
    "            if t != target:\n",
    "                return False, info_dict\n",
    "        \n",
    "        if line.startswith(FBS):\n",
    "            if not(subject in line and target in line):\n",
    "                return False, info_dict\n",
    "            \n",
    "            f = line[len(FBS) - 1: ]\n",
    "            f = f.strip()\n",
    "            \n",
    "            info_dict['f'] = f\n",
    "        \n",
    "        if line.startswith(MFBS):\n",
    "            mfbs_line = line\n",
    "            if not(subject in line):\n",
    "                return False, info_dict\n",
    "            \n",
    "            mf = line[len(MFBS) - 1: ]\n",
    "            mf = mf.strip()\n",
    "            \n",
    "            info_dict['mf'] = mf\n",
    "        \n",
    "        if line.startswith(NTBS):\n",
    "            nt = line[len(NTBS) - 1: ]\n",
    "            nt = nt.strip()\n",
    "            \n",
    "            info_dict['nt'] = nt\n",
    "            \n",
    "            if nt not in mfbs_line:\n",
    "                return False, info_dict\n",
    "            \n",
    "            if nt == target:\n",
    "                return False, info_dict\n",
    "            \n",
    "            if nt.lower() == subject.lower().strip():\n",
    "                return False, info_dict\n",
    "    \n",
    "    return True, info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16f615fe-bdd9-4cfe-aaca-b58752d599c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Info:  \n",
      "Question: Who was the lead actor in XXX?\n",
      "Subject: American Psycho\n",
      "Target: Christian Bale\n",
      "\n",
      "New Info: -\n",
      " Question: Who was the lead actor in XXX?\n",
      "Subject: American Psycho\n",
      "Target: Christian Bale\n",
      "Fact: The lead in American Psycho was actor Christian Bale.\n",
      "Modified Fact: The lead in American Psycho was actor Brad Pitt.\n",
      "New Target: Brad Pitt\n",
      "\n",
      "Is new info valid?  True\n"
     ]
    }
   ],
   "source": [
    "question = \"Who was the lead actor in XXX?\"\n",
    "subject = \"American Psycho\"\n",
    "answers = [\"Christian Bale\"]\n",
    "\n",
    "row = {'question': question, 'subject': subject, 'answers': answers}\n",
    "\n",
    "if validate_row(row):\n",
    "    sample_info = build_incomplete_info_block(row)\n",
    "\n",
    "    print(\"Sample Info: \", sample_info)\n",
    "    new_info = generate_complete_info(model, tokenizer, sample_info)\n",
    "    extracted_info = extract_info_block(new_info)\n",
    "    print(\"New Info: -\\n\", extracted_info)\n",
    "    \n",
    "    print()\n",
    "    print(\"Is new info valid? \", validate_info_and_extract_dict(extracted_info, row)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5756b8c8-ee85-4fbc-ba97-a6e8a117e0c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Info:  \n",
      "Question: In which fictional universe does XXX exist?\n",
      "Subject: Bronze Tiger\n",
      "Target: DC Universe\n",
      "\n",
      "New Info: -\n",
      " Question: In which fictional universe does XXX exist?\n",
      "Subject: Bronze Tiger\n",
      "Target: DC Universe\n",
      "Fact: Bronze Tiger exists in the DC Universe.\n",
      "Modified Fact: Bronze Tiger exists in the Marvel Universe.\n",
      "New Target: Marvel Universe\n",
      "\n",
      "Is new info valid?  True\n"
     ]
    }
   ],
   "source": [
    "question = \"In which fictional universe does XXX exist?\"\n",
    "subject = \"Bronze Tiger\"\n",
    "answers = [\"DC Universe\"]\n",
    "\n",
    "row = {'question': question, 'subject': subject, 'answers': answers}\n",
    "\n",
    "if validate_row(row):\n",
    "    sample_info = build_incomplete_info_block(row)\n",
    "\n",
    "    print(\"Sample Info: \", sample_info)\n",
    "    new_info = generate_complete_info(model, tokenizer, sample_info)\n",
    "    extracted_info = extract_info_block(new_info)\n",
    "    print(\"New Info: -\\n\", extracted_info)\n",
    "    \n",
    "    print()\n",
    "    print(\"Is new info valid? \", validate_info_and_extract_dict(extracted_info, row)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bace412c-a5fe-43cc-b87d-e9aa99fc053b",
   "metadata": {},
   "source": [
    "## BULK ATTEMPT\n",
    "Use previous code to build a dataset (CSV) of facts/counterfacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1bc2e7e-c6a1-4150-8da5-d609f060178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dparveez/Other\n"
     ]
    }
   ],
   "source": [
    "%cd /home/dparveez/Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6815582-a911-4314-a0e3-7d3522fd62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "DSET_FNAME = 'dset.csv'\n",
    "CSV_HEADER = ['Subject', 'Fact', 'Target', 'Counterfact', 'New Target', 'Question']\n",
    "NEW_DSET_SIZE = 200_000\n",
    "SAVE_SIZE = 25\n",
    "CURR_SIZE = 0\n",
    "\n",
    "dcsv = open(DSET_FNAME, mode='w', newline='')\n",
    "writer = csv.writer(dcsv)\n",
    "writer.writerow(CSV_HEADER)\n",
    "\n",
    "for row in dataset:\n",
    "    if validate_row(row):\n",
    "        info = build_incomplete_info_block(row)\n",
    "        # print(\"Incomplete Info: \", info)\n",
    "        \n",
    "        new_info = generate_complete_info(model, tokenizer, info)\n",
    "        extracted_info = extract_info_block(new_info)\n",
    "        # print(\"New Info: -\\n\", extracted_info)\n",
    "        # print()\n",
    "        \n",
    "        is_valid, edict = validate_info_and_extract_dict(extracted_info, row)\n",
    "    \n",
    "        if is_valid:\n",
    "            try:\n",
    "                csv_row = [row['subject'], edict['f'], row['answers'][0], edict['mf'], edict['nt'], row['question']]\n",
    "            except:\n",
    "                continue\n",
    "            writer.writerow(csv_row)\n",
    "            CURR_SIZE += 1\n",
    "        \n",
    "            if CURR_SIZE >= NEW_DSET_SIZE:\n",
    "                break\n",
    "            \n",
    "            if CURR_SIZE % SAVE_SIZE == 0:\n",
    "                dcsv.close()\n",
    "                dcsv = open(DSET_FNAME, mode='a', newline='')\n",
    "                writer = csv.writer(dcsv)\n",
    "\n",
    "dcsv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be3e229-a104-40de-8faf-1d6b6cb9b62f",
   "metadata": {},
   "source": [
    "## SAMPLE GENERATION - ALT APPROACH (NO SUCCESS YET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4deffcd6-d626-4142-9783-44d9056d5ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Answer:\n",
      "Answer: Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BASE_PROMPT = \"\"\"\n",
    "You are a helpful assistant who answers questions. Keep answers to the point.\n",
    "\n",
    "Examples: -\n",
    "Question: On what planet is Montes Teneriffe on?\n",
    "Answer: Moon.\n",
    "\n",
    "Question: Which was the creator of The Glo Friends?\n",
    "Answer: Hasbro.\n",
    "\"\"\"\n",
    "\n",
    "def generate_alt(model, tokenizer, question):\n",
    "    new_prompt = f\"\"\"{BASE_PROMPT}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(new_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=100, do_sample=True, top_p=0.9, temperature=1.2, num_return_sequences=5)\n",
    "    answers = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "    unique_answers = list(set(answers))  # Ensure uniqueness\n",
    "\n",
    "    return unique_answers[-1]\n",
    "\n",
    "\n",
    "print(generate_alt(model, tokenizer, \"What is the capital of France?\")[len(BASE_PROMPT)-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792feafb-a22f-4720-ae3c-5974f94d189a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
