{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20304902-c589-429c-94f7-13ae9ba39ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets==1.18.3 python-dotenv==0.19.2\n",
    "# !pip install tokenizers==0.19\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60069ca8-3f6c-443d-85cd-c5e89f45a705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov  9 20:09:31 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              45W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3fd9bf-787a-4bc3-b73f-fca98c0875ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 65039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07830d5f-161f-4cfe-9cea-3df7442cd242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/scratch/gilbreth/dparveez/'\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ebd0de7-d01b-49b0-a926-65c6f1604408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fde7f86-5d1c-469a-99ff-c916739e5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/scratch/gilbreth/dparveez/\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a68617-08b9-4c30-9412-59132a7ed8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/scratch/gilbreth/dparveez/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e2f21b-f2fa-4421-87b9-627028e5c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from util import nethook\n",
    "from util.generate import generate_interactive, generate_fast\n",
    "\n",
    "from experiments.py.demo import demo_model_editing, stop_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aee5aa1-b9aa-4197-94a0-44312b4e346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-xl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3692251b-d4ae-4f07-87e1-7094d9e3cda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2-xl\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 1600,\n",
       "  \"n_head\": 25,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 48,\n",
       "  \"n_positions\": 1024,\n",
       "  \"output_past\": true,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tok = (\n",
    "    AutoModelForCausalLM.from_pretrained(MODEL_NAME, low_cpu_mem_usage=False, cache_dir=BASE_PATH).to(\n",
    "        \"cuda\"\n",
    "    ),\n",
    "    AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=BASE_PATH),\n",
    ")\n",
    "tok.pad_token = tok.eos_token\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a598c7b3-12b5-4a27-8fe7-d1782fcfc9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pfw.txt', 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dea59af4-2b6b-4436-abf6-6e2a521d6c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust chunk_size based on model's max input size\n",
    "# Adjust stride based on desired overlap\n",
    "def chunkify(text, chunk_size=1024, stride=256):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), stride):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        if len(chunk) == chunk_size:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "text_chunks = chunkify(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff20c382-3791-40d2-886f-fbe66c6552f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_chunks = tok(text_chunks, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82257b0d-58c3-4b22-9220-027dc559276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = item['input_ids'].clone()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = TextDataset(encoded_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7625d62f-8d76-475a-aef6-8078f86d0999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(BASE_PATH, \"results\"),\n",
    "    num_train_epochs=75,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=750,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=os.path.join(BASE_PATH, \"logs\"),\n",
    "    logging_steps=50,\n",
    "    save_steps=500,                   # Save checkpoint every 500 steps\n",
    "    save_total_limit=1,\n",
    "    gradient_accumulation_steps=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb098f2c-0837-4380-b999-3025e740c0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b12289b-5ec3-41f1-94b1-a3db58b61741",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e37944-df6a-4255-b1e6-3314a3066c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 64/825 17:24 < 3:33:37, 0.06 it/s, Epoch 5.42/75]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.882900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fdda22-cccc-44cc-9675-ee044d780fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(os.path.join(BASE_PATH, \"fine_tuned_model_2/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979566c-df4e-43ed-8fac-3dddc0724d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.save_pretrained(os.path.join(BASE_PATH, \"fine_tuned_model_2/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08710e9-870c-40cc-be0c-61084b6fb368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3.9 (Default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
